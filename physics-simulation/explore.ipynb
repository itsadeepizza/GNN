{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "{DATASET_NAME} one of the datasets following the naming used in the paper:\n",
    "\n",
    "WaterDrop\n",
    "Water\n",
    "Sand\n",
    "Goop\n",
    "MultiMaterial\n",
    "RandomFloor\n",
    "WaterRamps\n",
    "SandRamps\n",
    "FluidShake\n",
    "FluidShakeBox\n",
    "Continuous\n",
    "WaterDrop-XL\n",
    "Water-3D\n",
    "Sand-3D\n",
    "Goop-3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "DATASET_NAME = \"WaterDrop\"\n",
    "BASE_URL=f\"https://storage.googleapis.com/learning-to-simulate-complex-physics/Datasets/{DATASET_NAME}/\"\n",
    "\n",
    "\n",
    "\n",
    "for file in [\"metadata.json\", \"train.tfrecord\", \"valid.tfrecord\", \"test.tfrecord\"]:\n",
    "    wget.download(f\"{BASE_URL}{file}\", \"dataset/water_drop\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/vahidk/tfrecord\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_path = \"dataset/water_drop/test.tfrecord\"\n",
    "metadata_path =  \"C:/Users/p.menegatti/PycharmProjects/GNN/physics-simulation/dataset/water_drop/metadata.json\"\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = f.read()\n",
    "index_path = None\n",
    "dataset = TFRecordDataset(train_path, index_path, metadata)\n",
    "print(\"Dataset loaded\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_37396\\3290235630.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtfrecord\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtfrecord_loader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mrecord\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mloader\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tfrecord\\reader.py\u001B[0m in \u001B[0;36mexample_loader\u001B[1;34m(data_path, index_path, description, shard, compression_type)\u001B[0m\n\u001B[0;32m    221\u001B[0m         \u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mParseFromString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[0mextract_feature_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdescription\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypename_mapping\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tfrecord\\reader.py\u001B[0m in \u001B[0;36mextract_feature_dict\u001B[1;34m(features, description, typename_mapping)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[0mprocessed_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 152\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdescription\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    153\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_keys\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Key {key} doesn't exist (select from {all_keys})!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import tfrecord\n",
    "\n",
    "loader = tfrecord.tfrecord_loader(train_path, None, metadata)\n",
    "for record in loader:\n",
    "    print(record)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_37396\\2768651230.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mT_co\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 68\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mNotImplementedError\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     69\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     70\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__add__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mother\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;34m'Dataset[T_co]'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;34m'ConcatDataset[T_co]'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_37396\\1766654814.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mloader\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    529\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 530\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    531\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    569\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 570\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    571\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    572\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     30\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m                     \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset_iter\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m                     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mended\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tfrecord\\reader.py\u001B[0m in \u001B[0;36mexample_loader\u001B[1;34m(data_path, index_path, description, shard, compression_type)\u001B[0m\n\u001B[0;32m    221\u001B[0m         \u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mParseFromString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrecord\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 223\u001B[1;33m         \u001B[1;32myield\u001B[0m \u001B[0mextract_feature_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdescription\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypename_mapping\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    224\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    225\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tfrecord\\reader.py\u001B[0m in \u001B[0;36mextract_feature_dict\u001B[1;34m(features, description, typename_mapping)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m     \u001B[0mprocessed_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 152\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtypename\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdescription\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    153\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_keys\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf\"Key {key} doesn't exist (select from {all_keys})!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "data = next(iter(loader))\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in converted code:\n    relative to C:\\Users:\n\n    p.menegatti\\AppData\\Local\\Temp\\ipykernel_4388\\596908444.py:8 list_record_features  *\n        for rec in tf.data.TFRecordDataset([str(tfrecords_path)]):\n    p.menegatti\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:648 reduce_body\n        body(iterate)\n    P02A0~1.MEN\\AppData\\Local\\Temp\\tmph0t7n83o.py:20 loop_body_1\n        example_bytes = ag__.converted_call(rec.numpy, list_record_features_scope.callopts, (), None, list_record_features_scope)\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4388\\596908444.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     50\u001B[0m         \u001B[0mwriter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexample\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSerializeToString\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[1;31m# Print extracted feature information\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 52\u001B[1;33m \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist_record_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtfrecords_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     53\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'\\n'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    447\u001B[0m     \u001B[1;31m# This is the first call of __call__, so we have to initialize.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    448\u001B[0m     \u001B[0minitializer_map\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobject_identity\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mObjectIdentityDictionary\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 449\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initialize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_initializers_to\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minitializer_map\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    450\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_created_variables\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    451\u001B[0m       \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_initialize\u001B[1;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[0;32m    390\u001B[0m     self._concrete_stateful_fn = (\n\u001B[0;32m    391\u001B[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001B[1;32m--> 392\u001B[1;33m             *args, **kwds))\n\u001B[0m\u001B[0;32m    393\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    394\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0minvalid_creator_scope\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0munused_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0munused_kwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_get_concrete_function_internal_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1845\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_signature\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1846\u001B[0m       \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1847\u001B[1;33m     \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1848\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m   2145\u001B[0m         \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2146\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mgraph_function\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2147\u001B[1;33m           \u001B[0mgraph_function\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_graph_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2148\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_cache\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprimary\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcache_key\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2149\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_create_graph_function\u001B[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m   2036\u001B[0m             \u001B[0marg_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0marg_names\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2037\u001B[0m             \u001B[0moverride_flat_arg_shapes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moverride_flat_arg_shapes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2038\u001B[1;33m             capture_by_value=self._capture_by_value),\n\u001B[0m\u001B[0;32m   2039\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_function_attributes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2040\u001B[0m         \u001B[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001B[0m\n\u001B[0;32m    913\u001B[0m                                           converted_func)\n\u001B[0;32m    914\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 915\u001B[1;33m       \u001B[0mfunc_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpython_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mfunc_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfunc_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    916\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    917\u001B[0m       \u001B[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36mwrapped_fn\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m    333\u001B[0m         \u001B[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    334\u001B[0m         \u001B[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 335\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mweak_wrapped_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__wrapped__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    336\u001B[0m     \u001B[0mweak_wrapped_fn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweakref\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mref\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwrapped_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    903\u001B[0m           \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint:disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    904\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"ag_error_metadata\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 905\u001B[1;33m               \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mag_error_metadata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    906\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    907\u001B[0m               \u001B[1;32mraise\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: in converted code:\n    relative to C:\\Users:\n\n    p.menegatti\\AppData\\Local\\Temp\\ipykernel_4388\\596908444.py:8 list_record_features  *\n        for rec in tf.data.TFRecordDataset([str(tfrecords_path)]):\n    p.menegatti\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_core\\python\\autograph\\operators\\control_flow.py:648 reduce_body\n        body(iterate)\n    P02A0~1.MEN\\AppData\\Local\\Temp\\tmph0t7n83o.py:20 loop_body_1\n        example_bytes = ag__.converted_call(rec.numpy, list_record_features_scope.callopts, (), None, list_record_features_scope)\n\n    AttributeError: 'Tensor' object has no attribute 'numpy'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def list_record_features(tfrecords_path):\n",
    "    # Dict of extracted feature information\n",
    "    features = {}\n",
    "    # Iterate records\n",
    "    for rec in tf.data.TFRecordDataset([str(tfrecords_path)]):\n",
    "        # Get record bytes\n",
    "        example_bytes = rec.numpy()\n",
    "        # Parse example protobuf message\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(example_bytes)\n",
    "        # Iterate example features\n",
    "        for key, value in example.features.feature.items():\n",
    "            # Kind of data in the feature\n",
    "            kind = value.WhichOneof('kind')\n",
    "            # Size of data in the feature\n",
    "            size = len(getattr(value, kind).value)\n",
    "            # Check if feature was seen before\n",
    "            if key in features:\n",
    "                # Check if values match, use None otherwise\n",
    "                kind2, size2 = features[key]\n",
    "                if kind != kind2:\n",
    "                    kind = None\n",
    "                if size != size2:\n",
    "                    size = None\n",
    "            # Save feature data\n",
    "            features[key] = (kind, size)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "test_path = \"dataset/water_drop/test.tfrecord\"\n",
    "\n",
    "tfrecords_path = test_path\n",
    "# Make some test records\n",
    "with tf.io.TFRecordWriter(tfrecords_path) as writer:\n",
    "    for i in range(10):\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(\n",
    "                feature={\n",
    "                    # Fixed length\n",
    "                    'id': tf.train.Feature(\n",
    "                        int64_list=tf.train.Int64List(value=[i])),\n",
    "                    # Variable length\n",
    "                    'data': tf.train.Feature(\n",
    "                        float_list=tf.train.FloatList(value=range(i))),\n",
    "                }))\n",
    "        writer.write(example.SerializeToString())\n",
    "# Print extracted feature information\n",
    "features = list_record_features(tfrecords_path)\n",
    "print(*features.items(), sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "This version of TensorFlow Datasets requires TensorFlow version >= 2.1.0; Detected an installation of version 1.15.0. Please upgrade TensorFlow to proceed.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4388\\2257593936.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     54\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 56\u001B[1;33m \u001B[0mprepare_data_from_tfds\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_4388\\2257593936.py\u001B[0m in \u001B[0;36mprepare_data_from_tfds\u001B[1;34m(data_path, is_rollout, batch_size)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;32mimport\u001B[0m \u001B[0mfunctools\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow_datasets\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtfds\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mimport\u001B[0m \u001B[0mreading_utils\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mimport\u001B[0m \u001B[0mtree\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_datasets\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;31m# needs to happen before anything else, since the imports below will try to\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;31m# import tensorflow, too.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow_datasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_compat\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m \u001B[0mtf_compat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_tf_install\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_datasets\\core\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtensorflow_datasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcore\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtf_compat\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m \u001B[0mtf_compat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_tf_install\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;31m# pylint:disable=g-import-not-at-top\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\rdkit\\lib\\site-packages\\tensorflow_datasets\\core\\tf_compat.py\u001B[0m in \u001B[0;36mensure_tf_install\u001B[1;34m()\u001B[0m\n\u001B[0;32m     62\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtf_version\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mmin_tf_version\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     raise ImportError(\n\u001B[1;32m---> 64\u001B[1;33m         \u001B[1;34m\"This version of TensorFlow Datasets requires TensorFlow \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m         \u001B[1;34mf\"version >= {MIN_TF_VERSION}; Detected an installation of version \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m         f\"{tf.__version__}. Please upgrade TensorFlow to proceed.\")\n",
      "\u001B[1;31mImportError\u001B[0m: This version of TensorFlow Datasets requires TensorFlow version >= 2.1.0; Detected an installation of version 1.15.0. Please upgrade TensorFlow to proceed."
     ]
    }
   ],
   "source": [
    "def prepare_data_from_tfds(data_path='data/train.tfrecord', is_rollout=False, batch_size=2):\n",
    "    import functools\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    import reading_utils\n",
    "    import tree\n",
    "    from tfrecord.torch.dataset import TFRecordDataset\n",
    "    def prepare_inputs(tensor_dict):\n",
    "        pos = tensor_dict['position']\n",
    "        pos = tf.transpose(pos, perm=[1, 0, 2])\n",
    "        target_position = pos[:, -1]\n",
    "        tensor_dict['position'] = pos[:, :-1]\n",
    "        num_particles = tf.shape(pos)[0]\n",
    "        tensor_dict['n_particles_per_example'] = num_particles[tf.newaxis]\n",
    "        if 'step_context' in tensor_dict:\n",
    "            tensor_dict['step_context'] = tensor_dict['step_context'][-2]\n",
    "            tensor_dict['step_context'] = tensor_dict['step_context'][tf.newaxis]\n",
    "        return tensor_dict, target_position\n",
    "    def batch_concat(dataset, batch_size):\n",
    "        windowed_ds = dataset.window(batch_size)\n",
    "        initial_state = tree.map_structure(lambda spec: tf.zeros(shape=[0] + spec.shape.as_list()[1:], dtype=spec.dtype),dataset.element_spec)\n",
    "        def reduce_window(initial_state, ds):\n",
    "            return ds.reduce(initial_state, lambda x, y: tf.concat([x, y], axis=0))\n",
    "        return windowed_ds.map(lambda *x: tree.map_structure(reduce_window, initial_state, x))\n",
    "    def prepare_rollout_inputs(context, features):\n",
    "        out_dict = {**context}\n",
    "        pos = tf.transpose(features['position'], [1, 0, 2])\n",
    "        target_position = pos[:, -1]\n",
    "        out_dict['position'] = pos[:, :-1]\n",
    "        out_dict['n_particles_per_example'] = [tf.shape(pos)[0]]\n",
    "        if 'step_context' in features:\n",
    "            out_dict['step_context'] = features['step_context']\n",
    "        out_dict['is_trajectory'] = tf.constant([True], tf.bool)\n",
    "        return out_dict, target_position\n",
    "\n",
    "    metadata_path =  \"C:/Users/p.menegatti/PycharmProjects/GNN/physics-simulation/dataset/water_drop/metadata.json\"\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = f.read()\n",
    "    ds = tf.data.TFRecordDataset([data_path])\n",
    "    ds = ds.map(functools.partial(reading_utils.parse_serialized_simulation_example, metadata=metadata))\n",
    "    if is_rollout:\n",
    "        ds = ds.map(prepare_rollout_inputs)\n",
    "    else:\n",
    "        split_with_window = functools.partial(\n",
    "            reading_utils.split_trajectory,\n",
    "            window_length=6 + 1)\n",
    "        ds = ds.flat_map(split_with_window)\n",
    "        ds = ds.map(prepare_inputs)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(512)\n",
    "        ds = batch_concat(ds, batch_size)\n",
    "    ds = tfds.as_numpy(ds)\n",
    "    for i in range(100): # clear screen\n",
    "        print()\n",
    "    return ds\n",
    "prepare_data_from_tfds()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}